#!/bin/bash
set -euo pipefail

# è¨ºæ‰€AIæŸ¥è©¢ç³»çµ± - ä¸€éµå®‰è£è…³æœ¬
# Author: Leon Lu
# Repository: https://github.com/leon80148/chat4lab

# ===================
# é¡è‰²å®šç¾©
# ===================
readonly RED='\033[0;31m'
readonly GREEN='\033[0;32m'
readonly YELLOW='\033[1;33m'
readonly BLUE='\033[0;34m'
readonly PURPLE='\033[0;35m'
readonly CYAN='\033[0;36m'
readonly NC='\033[0m' # No Color

# ===================
# æ—¥èªŒå‡½æ•¸
# ===================
log_info() {
    echo -e "${BLUE}â„¹ï¸  $1${NC}"
}

log_success() {
    echo -e "${GREEN}âœ… $1${NC}"
}

log_warning() {
    echo -e "${YELLOW}âš ï¸  $1${NC}"
}

log_error() {
    echo -e "${RED}âŒ $1${NC}"
}

log_step() {
    echo -e "${PURPLE}ğŸ”„ $1${NC}"
}

# ===================
# éŒ¯èª¤è™•ç†
# ===================
error_exit() {
    log_error "$1"
    exit 1
}

cleanup_on_exit() {
    if [[ $? -ne 0 ]]; then
        log_error "å®‰è£éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤ï¼Œæ­£åœ¨æ¸…ç†..."
        # æ¸…ç†è‡¨æ™‚æª”æ¡ˆ
        rm -rf temp_* 2>/dev/null || true
    fi
}

trap cleanup_on_exit EXIT

# ===================
# å·¥å…·å‡½æ•¸
# ===================
command_exists() {
    command -v "$1" >/dev/null 2>&1
}

get_os() {
    case "$OSTYPE" in
        linux-gnu*) echo "linux" ;;
        darwin*) echo "macos" ;;
        msys*|cygwin*) echo "windows" ;;
        *) echo "unknown" ;;
    esac
}

get_memory_gb() {
    local os=$(get_os)
    if [[ "$os" == "linux" ]]; then
        free -g | awk '/^Mem:/{print $2}'
    elif [[ "$os" == "macos" ]]; then
        echo $(($(sysctl -n hw.memsize) / 1024 / 1024 / 1024))
    else
        echo "unknown"
    fi
}

# ===================
# ç³»çµ±æª¢æŸ¥å‡½æ•¸
# ===================
check_system() {
    log_step "æª¢æŸ¥ç³»çµ±ç’°å¢ƒ..."
    
    local os=$(get_os)
    log_info "ä½œæ¥­ç³»çµ±: $os"
    
    if [[ "$os" == "unknown" ]]; then
        error_exit "ä¸æ”¯æ´çš„ä½œæ¥­ç³»çµ±: $OSTYPE"
    fi
    
    # æª¢æŸ¥è¨˜æ†¶é«”
    local memory_gb=$(get_memory_gb)
    if [[ "$memory_gb" != "unknown" ]]; then
        log_info "è¨˜æ†¶é«”: ${memory_gb}GB"
        if [[ $memory_gb -lt 16 ]]; then
            log_warning "è¨˜æ†¶é«”ä¸è¶³16GBï¼ŒLLMé‹è¡Œå¯èƒ½è¼ƒæ…¢"
        fi
    fi
    
    log_success "ç³»çµ±æª¢æŸ¥å®Œæˆ"
}

check_dependencies() {
    log_step "æª¢æŸ¥ä¾è³´è»Ÿé«”..."
    
    # æª¢æŸ¥Python
    if ! command_exists python3; then
        error_exit "Python 3 æœªå®‰è£ï¼Œè«‹å…ˆå®‰è£ Python 3.9+"
    fi
    
    local python_version=$(python3 -c 'import sys; print(".".join(map(str, sys.version_info[:2])))')
    log_info "Python ç‰ˆæœ¬: $python_version"
    
    # æª¢æŸ¥Pythonç‰ˆæœ¬
    if python3 -c "import sys; sys.exit(0 if sys.version_info >= (3,9) else 1)"; then
        log_success "Pythonç‰ˆæœ¬ç¬¦åˆè¦æ±‚"
    else
        error_exit "Pythonç‰ˆæœ¬éä½ï¼Œéœ€è¦3.9ä»¥ä¸Šç‰ˆæœ¬"
    fi
    
    # æª¢æŸ¥pip
    if ! command_exists pip3; then
        log_warning "pip3 æœªå®‰è£ï¼Œå˜—è©¦å®‰è£..."
        python3 -m ensurepip --upgrade || error_exit "pip3 å®‰è£å¤±æ•—"
    fi
    
    # æª¢æŸ¥Git
    if ! command_exists git; then
        error_exit "Git æœªå®‰è£ï¼Œè«‹å…ˆå®‰è£ Git"
    fi
    
    # æª¢æŸ¥curl
    if ! command_exists curl; then
        error_exit "curl æœªå®‰è£ï¼Œè«‹å…ˆå®‰è£ curl"
    fi
    
    # æª¢æŸ¥Docker (å¯é¸)
    if command_exists docker; then
        log_success "Docker å·²å®‰è£"
        export DOCKER_AVAILABLE=true
    else
        log_warning "Docker æœªå®‰è£ï¼Œå°‡ä½¿ç”¨æœ¬åœ°å®‰è£æ¨¡å¼"
        export DOCKER_AVAILABLE=false
    fi
    
    log_success "ä¾è³´æª¢æŸ¥å®Œæˆ"
}

# ===================
# å®‰è£å‡½æ•¸
# ===================
install_ollama() {
    log_step "å®‰è£ Ollama..."
    
    if command_exists ollama; then
        log_info "Ollama å·²å®‰è£ï¼Œæª¢æŸ¥ç‰ˆæœ¬..."
        ollama --version
        return
    fi
    
    local os=$(get_os)
    if [[ "$os" == "linux" || "$os" == "macos" ]]; then
        log_info "ä¸‹è¼‰ä¸¦å®‰è£ Ollama..."
        curl -fsSL https://ollama.ai/install.sh | sh
        log_success "Ollama å®‰è£å®Œæˆ"
    else
        log_warning "Windows ç³»çµ±éœ€è¦æ‰‹å‹•å®‰è£ Ollama"
        echo "è«‹è¨ªå•: https://ollama.ai/download/windows"
        read -p "å®‰è£å®Œæˆå¾ŒæŒ‰ Enter ç¹¼çºŒ..."
        
        if ! command_exists ollama; then
            error_exit "Ollama æœªæ­£ç¢ºå®‰è£"
        fi
    fi
}

setup_python_env() {
    log_step "è¨­ç½® Python è™›æ“¬ç’°å¢ƒ..."
    
    # å»ºç«‹è™›æ“¬ç’°å¢ƒ
    if [[ ! -d "venv" ]]; then
        log_info "å»ºç«‹è™›æ“¬ç’°å¢ƒ..."
        python3 -m venv venv
        log_success "è™›æ“¬ç’°å¢ƒå»ºç«‹å®Œæˆ"
    else
        log_info "è™›æ“¬ç’°å¢ƒå·²å­˜åœ¨"
    fi
    
    # å•Ÿå‹•è™›æ“¬ç’°å¢ƒ
    log_info "å•Ÿå‹•è™›æ“¬ç’°å¢ƒ..."
    source venv/bin/activate
    
    # å‡ç´špip
    log_info "å‡ç´š pip..."
    pip install --upgrade pip
    
    # å®‰è£ä¾è³´
    log_info "å®‰è£ Python ä¾è³´å¥—ä»¶..."
    if [[ -f "requirements.txt" ]]; then
        pip install -r requirements.txt
        log_success "ä¾è³´å®‰è£å®Œæˆ"
    else
        error_exit "requirements.txt æª”æ¡ˆä¸å­˜åœ¨"
    fi
}

setup_directories() {
    log_step "å»ºç«‹ç›®éŒ„çµæ§‹..."
    
    # å»ºç«‹å¿…è¦ç›®éŒ„
    mkdir -p data/{dbf_files,backups,sample} logs config/prompts
    
    # è¨­ç½®æ¬Šé™
    chmod 755 data logs config
    chmod 644 data/.gitkeep logs/.gitkeep 2>/dev/null || true
    
    log_success "ç›®éŒ„çµæ§‹å»ºç«‹å®Œæˆ"
}

setup_config() {
    log_step "è¨­ç½®é…ç½®æª”æ¡ˆ..."
    
    # è¤‡è£½ç’°å¢ƒè®Šæ•¸ç¯„ä¾‹
    if [[ ! -f ".env" ]]; then
        if [[ -f ".env.example" ]]; then
            cp .env.example .env
            log_success "ç’°å¢ƒè®Šæ•¸æª”æ¡ˆå»ºç«‹å®Œæˆ"
        else
            log_warning ".env.example æª”æ¡ˆä¸å­˜åœ¨"
        fi
    else
        log_info ".env æª”æ¡ˆå·²å­˜åœ¨"
    fi
    
    # è¤‡è£½ç³»çµ±è¨­å®šç¯„ä¾‹
    if [[ ! -f "config/settings.yaml" ]]; then
        if [[ -f "config/settings.yaml.example" ]]; then
            cp config/settings.yaml.example config/settings.yaml
            log_success "ç³»çµ±è¨­å®šæª”æ¡ˆå»ºç«‹å®Œæˆ"
        else
            log_warning "config/settings.yaml.example æª”æ¡ˆä¸å­˜åœ¨"
        fi
    else
        log_info "config/settings.yaml æª”æ¡ˆå·²å­˜åœ¨"
    fi
}

start_ollama_service() {
    log_step "å•Ÿå‹• Ollama æœå‹™..."
    
    # æª¢æŸ¥æœå‹™æ˜¯å¦å·²åœ¨é‹è¡Œ
    if curl -s http://localhost:11434/api/tags >/dev/null 2>&1; then
        log_info "Ollama æœå‹™å·²åœ¨é‹è¡Œ"
        return
    fi
    
    # å•Ÿå‹•æœå‹™
    log_info "å•Ÿå‹• Ollama èƒŒæ™¯æœå‹™..."
    nohup ollama serve > logs/ollama.log 2>&1 &
    
    # ç­‰å¾…æœå‹™å•Ÿå‹•
    local timeout=30
    local count=0
    while [[ $count -lt $timeout ]]; do
        if curl -s http://localhost:11434/api/tags >/dev/null 2>&1; then
            log_success "Ollama æœå‹™å•Ÿå‹•æˆåŠŸ"
            return
        fi
        sleep 1
        ((count++))
    done
    
    error_exit "Ollama æœå‹™å•Ÿå‹•è¶…æ™‚"
}

download_model() {
    log_step "ä¸‹è¼‰ Gemma2 æ¨¡å‹..."
    
    # æª¢æŸ¥æ¨¡å‹æ˜¯å¦å·²å­˜åœ¨
    if ollama list | grep -q "gemma2:9b-instruct-q4_0"; then
        log_info "æ¨¡å‹å·²å­˜åœ¨ï¼Œè·³éä¸‹è¼‰"
        return
    fi
    
    log_info "é–‹å§‹ä¸‹è¼‰æ¨¡å‹ (ç´„5.4GBï¼Œå¯èƒ½éœ€è¦è¼ƒé•·æ™‚é–“)..."
    
    # é¡¯ç¤ºé€²åº¦
    ollama pull gemma2:9b-instruct-q4_0 &
    local pid=$!
    
    # ç°¡å–®çš„é€²åº¦æŒ‡ç¤º
    while kill -0 $pid 2>/dev/null; do
        echo -n "."
        sleep 2
    done
    echo
    
    wait $pid
    local exit_code=$?
    
    if [[ $exit_code -eq 0 ]]; then
        log_success "æ¨¡å‹ä¸‹è¼‰å®Œæˆ"
    else
        error_exit "æ¨¡å‹ä¸‹è¼‰å¤±æ•—"
    fi
}

initialize_database() {
    log_step "åˆå§‹åŒ–è³‡æ–™åº«..."
    
    if [[ -f "scripts/setup_db.py" ]]; then
        log_info "åŸ·è¡Œè³‡æ–™åº«åˆå§‹åŒ–è…³æœ¬..."
        python scripts/setup_db.py --create-schema
        log_success "è³‡æ–™åº«åˆå§‹åŒ–å®Œæˆ"
    else
        log_warning "è³‡æ–™åº«åˆå§‹åŒ–è…³æœ¬ä¸å­˜åœ¨ï¼Œè«‹ç¨å¾Œæ‰‹å‹•åŸ·è¡Œ"
    fi
}

run_health_check() {
    log_step "åŸ·è¡Œç³»çµ±å¥åº·æª¢æŸ¥..."
    
    # æª¢æŸ¥Ollamaæœå‹™
    if curl -s http://localhost:11434/api/tags >/dev/null 2>&1; then
        log_success "âœ“ Ollama æœå‹™æ­£å¸¸"
    else
        log_error "âœ— Ollama æœå‹™ç•°å¸¸"
    fi
    
    # æª¢æŸ¥æ¨¡å‹
    if ollama list | grep -q "gemma2:9b-instruct-q4_0"; then
        log_success "âœ“ æ¨¡å‹è¼‰å…¥æ­£å¸¸"
    else
        log_error "âœ— æ¨¡å‹æœªæ­£ç¢ºè¼‰å…¥"
    fi
    
    # æª¢æŸ¥Pythonç’°å¢ƒ
    if python -c "import streamlit, pandas, ollama" 2>/dev/null; then
        log_success "âœ“ Pythonä¾è³´æ­£å¸¸"
    else
        log_error "âœ— Pythonä¾è³´ç¼ºå¤±"
    fi
    
    # åŸ·è¡Œè©³ç´°å¥åº·æª¢æŸ¥
    if [[ -f "scripts/health_check.py" ]]; then
        log_info "åŸ·è¡Œè©³ç´°å¥åº·æª¢æŸ¥..."
        python scripts/health_check.py
    fi
}

show_completion_message() {
    echo
    echo "ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰"
    log_success "è¨ºæ‰€AIæŸ¥è©¢ç³»çµ±å®‰è£å®Œæˆï¼"
    echo "ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰"
    echo
    echo "ğŸ¯ å•Ÿå‹•æŒ‡ä»¤:"
    echo "   source venv/bin/activate      # å•Ÿå‹•è™›æ“¬ç’°å¢ƒ"
    echo "   streamlit run src/app.py      # å•Ÿå‹•æ‡‰ç”¨ç¨‹å¼"
    echo
    echo "ğŸŒ è¨ªå•ç¶²å€:"
    echo "   http://localhost:8501"
    echo
    echo "ğŸ³ Docker å¿«é€Ÿå•Ÿå‹• (å¦‚æœæ”¯æ´):"
    if [[ "$DOCKER_AVAILABLE" == "true" ]]; then
        echo "   docker-compose up -d          # å•Ÿå‹•æ‰€æœ‰æœå‹™"
        echo "   docker-compose logs -f        # æŸ¥çœ‹æ—¥èªŒ"
    else
        echo "   (Docker æœªå®‰è£ï¼Œè«‹ä½¿ç”¨ä¸Šæ–¹çš„æœ¬åœ°å•Ÿå‹•æ–¹å¼)"
    fi
    echo
    echo "ğŸ“– èªªæ˜æ–‡æª”:"
    echo "   - å®‰è£æŒ‡å—: docs/installation.md"
    echo "   - é…ç½®èªªæ˜: docs/configuration.md"
    echo "   - ä½¿ç”¨æ‰‹å†Š: README.md"
    echo
    echo "ğŸ†˜ éœ€è¦å¹«åŠ©ï¼Ÿ"
    echo "   - GitHub Issues: https://github.com/leon80148/chat4lab/issues"
    echo "   - è¯çµ¡ä½œè€…: leon80148@gmail.com"
    echo
}

show_banner() {
    echo -e "${CYAN}"
    cat << "EOF"
    ____  _            _        ___    _____ 
   / ___|| |          (_)      / _ \  |_   _|
  | |    | |    _   _  _  __  | |_| |   | |  
  | |___ | |   | | | || ||  | |  _  |   | |  
   \____||_|   |_| |_||_||__|_|_| |_|  |_|  
                                              
   è¨ºæ‰€ AI æŸ¥è©¢ç³»çµ± - Clinic AI Query System
   
   ğŸ¥ æ™ºèƒ½é†«ç™‚è³‡æ–™æŸ¥è©¢ï¼Œè®“æ•¸æ“šèªªè©±ï¼
   ğŸ¤– æœ¬åœ°LLMéƒ¨ç½²ï¼Œè³‡æ–™å®‰å…¨ç„¡æ†‚ï¼
   ğŸ“Š è‡ªç„¶èªè¨€æŸ¥è©¢ï¼Œç°¡å–®æ˜“ç”¨ï¼
   
EOF
    echo -e "${NC}"
}

# ===================
# ä¸»è¦å®‰è£æµç¨‹
# ===================
main() {
    show_banner
    
    log_info "é–‹å§‹å®‰è£è¨ºæ‰€AIæŸ¥è©¢ç³»çµ±..."
    echo "================================"
    
    # æª¢æŸ¥éšæ®µ
    check_system
    check_dependencies
    
    # å®‰è£éšæ®µ
    install_ollama
    setup_python_env
    setup_directories
    setup_config
    
    # æœå‹™å•Ÿå‹•éšæ®µ
    start_ollama_service
    download_model
    initialize_database
    
    # é©—è­‰éšæ®µ
    run_health_check
    
    # å®Œæˆæç¤º
    show_completion_message
}

# åŸ·è¡Œä¸»å‡½æ•¸
if [[ "${BASH_SOURCE[0]}" == "${0}" ]]; then
    main "$@"
fi